%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Authors:
% Wei Hu (whu061@uottawa.ca)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands

% Theorems Format
\usepackage{amsthm}
% Fractions Format
\usepackage{xfrac}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}

% Floor Function
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

%----------------------------------------------------------------------------------------
% HEADER
%----------------------------------------------------------------------------------------

\title{\underline{CSI 4900}: Honours Project Report} % Title of the assignment

\author{Wei Hu\\ \texttt{whu061@uottawa.ca}} % Author name and email address

\date{University of Ottawa --- \today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
% INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction} % Unnumbered section

An Erd\"os-Selfridge-Spencer game consists of an attacker $A$, a defender $D$, and a game state represented by an array $S$, where $S[i]$ is the number of pieces on the board that is $i$ levels away from the top of the board. The board is zero-index so that a piece which is at level $0$ will attain tenure and provide a point for the attacker, if not destroyed by the defender this turn.

\begin{defn}
  Let $v$ be a value function, which maps a level $i$ to a real number representing the value of a piece at level $i$. A value function can be represented by an array $(w_{0}, w_{1}, w_{2}, ... , w_{k})$ where:
  \begin{equation*}
    v(i) = w_{i}.
  \end{equation*}
\end{defn}

\begin{defn}
  Let $S$ be a state on a board with $k$ levels. The value function applied to $S$ is defined as:
  \begin{equation*}
    v(S) = \sum_{i = 0}^k S[i]v(i).
  \end{equation*}
\end{defn}

A defender would apply its value function to the two sets partitioned by the attackers, and it would destroy the set it deems to be more valuable.

\begin{thm}
  The optimal value function $v_{*}$ for the defender is defined as:
  % Math equation/formula
  \begin{equation}
    v_{*}(i) = \frac{1}{2^{i+1}}.
  \end{equation}
\end{thm}

%----------------------------------------------------------------------------------------
% BIASED DEFENDERS
%----------------------------------------------------------------------------------------

\section{Biased Defenders} % Numbered section

We observe that the optimal value function satisfies the property that $v(i) = 2v(i+1)$. By deviating from this equality, we can create sub-optimal defenders. These defenders either overvalue pieces that are close to the top of the board, or those that are the furthest away. This bias is dependent on the direction of the inequality.

\begin{defn}
  A farsighted defender is a defender whose values function satisfies:
  \begin{equation}
    v(i) < 2v(i+1).
  \end{equation}
  for all $0 <= i < k$, where $k$ is the length of the board.
\end{defn}

A farsighted defender disproportionally value pieces that are close to the bottom of the board. \\

\begin{defn}
  A nearsighted (myopic) defender is a defender whose value function satisfies:
  \begin{equation}
    v(i) > 2v(i+1).
  \end{equation}
  for all $0 <= i < k$, where $k$ is the length of the board.
\end{defn}

A nearsighted defender will overvalue pieces that are near the top of the board.

%------------------------------------------------

\section{Project Summary}

We intend use various reinforcement learning (RL) techniques to train attackers to exploit the sub-optimality of biased defenders, both nearsighted and farsighted. We observe that playing optimally against a biased defender is more complicated than playing optimally against an optimal defender:

\begin{thm}
  When playing against an optimal defender, the optimal approach for the attacker is to try to make the value according to $v_{*}$ of the two partitioned sets as close as possible.
\end{thm}

This approach maximizes, at each turn, the value according to $v_{*}$ of the surviving pieces. A linear algorithm exists for finding the partition described in Theorem 3.1. The theorem can be proved by:
\begin{enumerate}
  \item Showing that when playing against an optimal defender, the best score that can be achieved for a starting state $S$ is $\floor{v_{*}(S)}$.
  \item Showing that playing according to Theorem 3.1 guarantees that we will score at least $\floor{v_{*}(S)}$.
\end{enumerate}

However, when playing a biased defender, it is possible for the real value of the surviving subset to be larger than that of the one destroyed. Due to its bias, a set could have higher value than another set according to $v_{*}$, but be deemed less valuable according to the defender's $v$. As a result, we can be certain that the approach described in Theorem 3.1 is not optimal.\\

The idea here is to examine how various RL approaches such as Q-learning, policy gradient, and actor critic can "discover" more optimal approaches against these defenders.

\subsection{Challenges}
In the original paper first introducing Erd\"os-Selfridge-Spencer games as test environments for RL algorithms (Raghu et al. 2017), the focus was mainly on training defender agents. Defenders have an action space of size two, while the attackers, which is what we will mainly be training, has an $O(2^n)$ action space.\\

While the paper provides a method to train attackers, it employs a techniques through which the attacker's action space is made linear. This reduction not only substantially increased the proportion of optimal moves (making the game easier for the attacker), but the reduced action space is also only guaranteed to contain the optimal move against the optimal defender. \\

Furthermore, rewards are sparse, and we only get feedback whenever a piece gets tenure. Since most of the attacker's action are bad and lead to no reward, it can be difficult to gather good training feedback.
%------------------------------------------------
\section{Algorithms }
We were able to analytically prove properties about biased defenders. We provide two algorithms which maximizes the real value of the surviving pieces at the end of each turn against any nearsighted or farsighted defender.\\

Two core ideas behind the algorithms  are laid out in the following lemmas:

\begin{lem}
  If a set of pieces $S$ which are all further away from tenure than a piece at level $i$ has $v_{*}(S) >= v_{*}(i)$, then there is a subset $S'$ of $S$ that satisfies $v_{*}(S') = v_{*}(i)$.
\end{lem}


\begin{proof}
 Showing lemma 4.1 is equivalent to showing that for a fraction $p = \sfrac{1}{2^i}$ and a multiset $M$ of fractions which contains only elements of the form $\sfrac{1}{2^{i+j}}$ where $j>0$. If the elements in $M$ adds up to a value greater than $p$, then a subset of $M$ adds up to exactly $p$. To prove, we first create a new multiset $M'$, by dividing each element in $M$ by $p$. Notice the problem is equivalent to showing that a subset of $M'$ adds up to 1. Since, the sum of $M'$ must be greater than 1, and $M'$ contains only powers of $\sfrac{1}{2}$, lemma 1.X completes the proof.
\end{proof}

\begin{lem}
  Let $v$ be the value function of a farsighted defender. Let $A$, $B$ be two sets of pieces. If every piece in $B$ is closer to tenure than any piece in $A$, and  $B$ has greater value according to $v$, then $B$ has greater value according to $v_{*}$ as well.
\end{lem}

\begin{proof}
 If $A$ is greater than $B$ according $v*$, then we can continually apply lemma 4.1, and map each piece in $B$ to a subset of $A$, never reusing the same piece from $A$ more than once (and there will be pieces left over in $A$, not mapped to anything in $B$). In each of these mappings, each piece from the subset of $A$ will be valued according to $v*$ as being worth $\sfrac{1}{2^i}$ of the value of the piece in $B$, where $i$ is the number of levels the piece in $A$ is away from the piece in $B$. According to $v$, however, the same piece would be worth more than $\sfrac{1}{2^i}$ the value of the piece in $B$, since $v(i) < 2v(i+1)$ for a farsighted defender. Thus, the pieces in the set from $A$ will be valued less by $v$ than the piece in $B$. Applying this to every set in the mapping, we conclude $A$ will be valued higher than $B$ by $v$. We complete the proof by contraposition.
\end{proof}

Similarly, we also have:

\begin{lem}
Let $v$ be the value function of a nearsighted defender. Let $A$, $B$ be two sets of pieces. If every piece in $B$ is further from tenure than any piece in $A$, and $B$ has greater value according to $v$, then $B$ has greater value according to $v_{*}$ as well.
\end{lem}

%------------------------------------------------

\subsection{Farsighted Defenders}

\begin{thm}
  The following algorithm maximizes the real value of the surviving pieces at the end of each turn against any farsighted defender.
\end{thm}

\begin{center}
  \begin{minipage}{1\linewidth} % Adjust the minipage width to accomodate for the length of algorithm lines
    \begin{algorithm}[H]
      \KwIn{a board position $S$, and the value function of a farsighted defender $v$}  % Algorithm inputs
      \KwResult{$(c, d)$, such that $a+b = c + d$} % Algorithm outputs/results
      \medskip
      \If{$\vert b\vert > \vert a\vert$}{
        exchange $a$ and $b$ \;
      }
      $z \leftarrow c - a$ \;
      $d \leftarrow b - z$ \;
      {\bf return} $(c,d)$ \;
      \caption{\texttt{Minimizing $v_{*}$ Value of Destroyed Set Against Farsighted Defenders}} % Algorithm name
      \label{alg:farsighted}   % optional label to refer to
    \end{algorithm}
  \end{minipage}
\end{center}

\begin{proof}

\end{proof}

\newpage

\subsection{Nearsighted Defenders}

\begin{thm}
  The following algorithm maximizes the real value of the surviving pieces at the end of each turn against any nearsighted defender.
\end{thm}

\begin{center}
  \begin{minipage}{1\linewidth} % Adjust the minipage width to accomodate for the length of algorithm lines
    \begin{algorithm}[H]

      \caption{\texttt{Minimizing $v_{*}$ Value of Destroyed Set Against Nearsighted Defenders}} % Algorithm name
      \label{alg:nearsighted}   % optional label to refer to

      \KwIn{a board position $S$ with $k$ levels, and the value function of a nearsighted defender $v$}  % Algorithm inputs
      \KwResult{ a partition of the board $(S_{1}, S_{2})$ which guarantees the subset that the defender will destroy has the lowest possible value according to $v_{*}$} % Algorithm outputs/results

      \medskip
      $\gamma \leftarrow \frac{v(S)}{2}$\;
      $best\_set \leftarrow S$ \;
      $best\_value \leftarrow v_{*}(S)$ \;
      $current\_set \leftarrow (0, 0, 0, ...)$ \;
      $current\_value \leftarrow 0$ \;

      \For{$i \leftarrow 0$ to $k$} {
        \For{$piece \leftarrow 0$ to $S[i]$} {
        		\If{$v(i) < \gamma$}{
        			 $current\_set[i]{+}{+}$\;
        			 $\gamma {-}{=} v(i)$\;
        			 $current\_value {+}{=} v_{*}(i)$\;
      		}
      		\Else{
      			$temp  \leftarrow current\_set$\;
      			$temp[i]{+}{+}$\;
      			\If{$current $}{
        			 $best\_set \leftarrow temp$ \;
      		}
      		$i{+}{+}$ \;
      		}
        }
      }

      {\bf return} $(best\_set, (P - best\_set))$ \;
    \end{algorithm}
  \end{minipage}
\end{center}

\begin{proof}
The idea behind the algorithm is that as we iterate through the board, starting with the more valuable pieces (according to $v_{*}$), we add to current\_set the pieces we are certain...


\end{proof}


%----------------------------------------------------------------------------------------
\section{Machine Learning}
\subsection{Q-Learning}
The traditional Q-learning approach using a Q-table is only feasible for small tables.
\subsubsection{Deep Q-Learning}
We propose a method for using a neural network to represent the Q table, using a method to compress the action space?

\subsection{Policy Gradient}
\subsection{Actor Critic}

%----------------------------------------------------------------------------------------

\section{Observations}
\section{Future Work}
Find proof for actual optimal play. Is playing "greedily" every turn really the best approach?

\end{document}

%----------------------------------------------------------------------------------------
% TO DO LIST

% Introduce ESS Games, and mention how our variety cares about how many pieces get promoted rather than whether any piece gets promoted.
% Proofs about random defenders, splitting lemma etc (from previous type-up)
% Add bibliography.

%----------------------------------------------------------------------------------------
% TEMPLATE ELEMENTS
%----------------------------------------------------------------------------------------

% Numbered question, with subquestions in an enumerate environment
%\begin{question}
% Quisque ullamcorper placerat ipsum. Cras nibh. Morbi vel justo vitae lacus tincidunt ultrices. Lorem ipsum dolor sit amet, consectetuer adipiscing elit.
%
% % Subquestions numbered with letters
% \begin{enumerate}[(a)]
%   \item Do this.
%   \item Do that.
%   \item Do something else.
% \end{enumerate}
%\end{question}

% Numbered question, with an optional title
%\begin{question}[\itshape (with optional title)]
% In congue risus leo, in gravida enim viverra id. Donec eros mauris, bibendum vel dui at, tempor commodo augue. In vel lobortis lacus. Nam ornare ullamcorper mauris vel molestie. Maecenas vehicula ornare turpis, vitae fringilla orci consectetur vel. Nam pulvinar justo nec neque egestas tristique. Donec ac dolor at libero congue varius sed vitae lectus. Donec et tristique nulla, sit amet scelerisque orci. Maecenas a vestibulum lectus, vitae gravida nulla. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.
%\end{question}

% File contents
%\begin{file}[hello.py]
%\begin{lstlisting}[language=Python]
%#! /usr/bin/python
%
%import sys
%sys.stdout.write("Hello World!\n")
%\end{lstlisting}
%\end{file}

% Warning text, with a custom title
%\begin{warn}[Notice:]
%  In congue risus leo, in gravida enim viverra id. Donec eros mauris, bibendum vel dui at, tempor commodo augue. In vel lobortis lacus. Nam ornare ullamcorper mauris vel molestie. Maecenas vehicula ornare turpis, vitae fringilla orci consectetur vel. Nam pulvinar justo nec neque egestas tristique. Donec ac dolor at libero congue varius sed vitae lectus. Donec et tristique nulla, sit amet scelerisque orci. Maecenas a vestibulum lectus, vitae gravida nulla. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.
%\end{warn}

% Command-line "screenshot"
%\begin{commandline}
% \begin{verbatim}
%   $ chmod +x hello.py
%   $ ./hello.py
%
%   Hello World!
% \end{verbatim}
%\end{commandline}